1.为什么搜索是近实时的？
磁盘在这里成为了瓶颈。提交（Commiting）一个新的段到磁盘需要一个 fsync 来确保段被物理性地写入磁盘，
这样在断电的时候就不会丢失数据。 但是 fsync 操作代价很大; 如果每次索引一个文档都去执行一次的话会造成很大的性能问题。

我们需要的是一个更轻量的方式来使一个文档可被搜索，这意味着 fsync 要从整个过程中被移除。

在Elasticsearch和磁盘之间是文件系统缓存。 像之前描述的一样， 在内存索引缓冲区中的文档会被写入到一个新的段中。 
但是这里新段会被先写入到  文件系统缓存  —​这一步代价会比较低，稍后再被刷新到磁盘—​这一步代价比较高。
不过只要文件已经在缓存中，就可以像其它文件一样被打开和读取了。

2.为什么文档的 CRUD (创建-读取-更新-删除) 操作是 实时 的?	
3.Elasticsearch 是怎样保证更新被持久化在断电时也不丢失数据?

4.为什么删除文档不会立刻释放空间？
删除和更新
段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 
取而代之的是，每个提交点会包含一个 .del 文件，文件中会列出这些被删除文档的段信息。
当一个文档被 “删除” 时，它实际上只是在 .del 文件中被 标记 删除。一个被标记删除的文档仍然可以被查询匹配到， 
但它会在最终结果被返回前从结果集中移除。

文档更新也是类似的操作方式：当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 
可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。

5.refresh, flush, 和 optimize API 都做了什么, 你什么情况下应该使用他们？
在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh 。 
默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 近 实时搜索: 文档的变化并不是立即对搜索可见，
但会在一秒之内变为可见。
这些行为可能会对新用户造成困惑: 他们索引了一个文档然后尝试搜索它，但却没有搜到。
这个问题的解决办法是用 refresh API 执行一次手动刷新:



分析与分析器？
分析器实际上是将三个功能封装在了一个包里。
1）字符过滤器
首先，字符串按顺序通过每个字符过滤器。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉html，或者将&转化成and。
2）分词器
其次，字符串被分词器分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条
3）Token过滤器
最后，词条按顺序通过每个token过滤器。这个过程可能会改变词条（例如，小写化；Quick），删除词条（例如，像a,and,the等无用词）。
或者增加词条（例如，像jump和leap这种同义词）。


索引与分片的比较

被混淆的概念是，一个 Lucene 索引 我们在 Elasticsearch 称作 分片 。 一个 Elasticsearch 索引 是分片的集合。 
当 Elasticsearch 在索引中搜索的时候， 他发送查询到每一个属于索引的分片(Lucene 索引)，然后像 执行分布式检索 提到的那样，
合并每个分片的结果到一个全局的结果集。



持久化变更？
虽然现在内存buffer中的索引不是直接提交到磁盘，而是先写入文件系统缓存中的segment file来达到近实时搜索。但是还是需要提交之后才能
写入磁盘，避免丢失数据。但是两次提交之间如果宕机则仍会丢失数据。
提供了translog或者叫事务日志，在每一次对 Elasticsearch 进行操作时均进行了日志记录。
新的文档被添加到内存缓冲区，同时也把这次操作写入translog。
在每秒的刷新操作之后，内存缓冲区里的数据被写入到文件系统缓存一个新的段中，且没有进行 fsync 操作。
此时内存缓冲区会被清空。但是translog还会保留。
translog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。当 Elasticsearch 启动的时候， 
它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放 translog 中所有在最后一次提交后发生的变更操作。

**translog 也被用来提供实时 CRUD 。当你试着通过ID查询、更新、删除一个文档，
它会在尝试从相应的段中检索之前， 首先检查 translog 任何最近的变更。
这意味着它总是能够实时地获取到文档的最新版本。
Translog 有多安全?

translog 的目的是保证操作不会丢失。这引出了这个问题： Translog 有多安全？

在文件被 fsync 到磁盘前，被写入的文件在重启之后就会丢失。默认 translog 是每 5 秒被 fsync 刷新到硬盘， 
或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)。这个过程在主分片和复制分片都会发生。
最终， 基本上，这意味着在整个请求被 fsync 到主分片和复制分片的translog之前，你的客户端不会得到一个 200 OK 响应。

在每次请求后都执行一个 fsync 会带来一些性能损失，尽管实践表明这种损失相对较小
（特别是bulk导入，它在一次请求中平摊了大量文档的开销）。

但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群，使用异步的 fsync 还是比较有益的。
比如，写入的数据被缓存到内存中，再每5秒执行一次 fsync 。

这个行为可以通过设置 durability 参数为 async 来启用：


段合并？
由于自动刷新流程每秒会创建一个新的段（这个新段存在于文件系统缓存中） ，这样会导致短时间内的段数量暴增。
而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和cpu运行周期。
更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。

Elasticsearch通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。

段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。




###路由一个文档到一个分片中
当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？
当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？
首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。
实际上，这个过程是根据下面这个公式决定的：
shard = hash(routing) % number_of_primary_shards
routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，
然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数 。
这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。
这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，
那么所有之前路由的值都会无效，文档也再也找不到了。
